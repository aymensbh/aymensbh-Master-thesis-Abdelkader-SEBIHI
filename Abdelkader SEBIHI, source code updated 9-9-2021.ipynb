{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pydot\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.xkcd()\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Input, Embedding, Reshape, Activation, Dropout, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform, he_uniform\n",
    "\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import keras\n",
    "import pydot as pyd\n",
    "\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating = pd.read_csv('http://files.grouplens.org/datasets/movielens/ml-100k/u1.base', sep = '\\t', engine='python', header=None)\n",
    "df_rating.columns = ['UserId', 'MovieId', 'Rating', 'Timestamp']\n",
    "df_rating_test = pd.read_csv('http://files.grouplens.org/datasets/movielens/ml-100k/u1.test', sep = '\\t', engine='python', header=None)\n",
    "\n",
    "df_users = pd.read_csv('http://files.grouplens.org/datasets/movielens/ml-100k/u.user', sep = '|', engine='python', header=None)\n",
    "df_users.columns = ['UserId', 'Age', 'Gender', 'Occupation', 'ZipCode']\n",
    "df_users.set_index('UserId', inplace = True)\n",
    "df_items = pd.read_csv('http://files.grouplens.org/datasets/movielens/ml-100k/u.item', sep = '|', engine='python', encoding ='ISO-8859-1', header=None)\n",
    "df_items.columns = ['MovieId', 'Title', 'Date', 'VideoReleaseDate', 'Url', 'unknown', 'Action','Adventure', 'Animation', \n",
    "                    'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy','Film-Noir', 'Horror','Musical', 'Mystery', 'Romance', \n",
    "                    'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "df_items.set_index('MovieId', inplace = True)\n",
    "\n",
    "df_matrix = df_rating.pivot(index='UserId', columns='MovieId', values='Rating')\n",
    "\n",
    "n_users = len(df_users)\n",
    "n_items = len(df_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(df, anchor):\n",
    "    POS_THR = 4.0\n",
    "    \n",
    "    ps = df.loc[anchor] >= POS_THR \n",
    "    return ps[ps].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg(df, anchor):\n",
    "    NEG_THR = 3.0\n",
    "    \n",
    "    ps = df.loc[anchor] <= NEG_THR \n",
    "    return ps[ps].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_con_features = [\"Age\"]\n",
    "user_cat_features = [\"Gender\", \"Occupation\"]\n",
    "\n",
    "item_features = ['Action','Adventure', 'Animation', \n",
    "                'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', \n",
    "                'Fantasy','Film-Noir', 'Horror','Musical', 'Mystery', 'Romance', \n",
    "                'Sci-Fi', 'Thriller', 'War', 'Western']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(df, con_features, cat_features):\n",
    "    # initialise an empty lists of vectors\n",
    "    X_cat = []\n",
    "    X_con = []\n",
    "\n",
    "    # for each continuous feature, add it as it is.\n",
    "    for column in con_features:\n",
    "        X = np.array(df[column])\n",
    "        X_con.append(X)\n",
    "\n",
    "    # for each categorical feature, get the numerical encoding of the feature vector\n",
    "    for column in cat_features:\n",
    "        X = np.asarray(df[column].tolist())\n",
    "        X_line = pd.factorize(X)[0]\n",
    "        X_cat.append(np.asarray(X_line))\n",
    "    \n",
    "    # transform lists in arrays\n",
    "    try:\n",
    "        X_con = np.column_stack(X_con)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:    \n",
    "        X_cat = np.column_stack(X_cat)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # concatenate arrays\n",
    "    if (len(cat_features) > 0) and (len(con_features) > 0): \n",
    "        X = np.concatenate((X_con, X_cat), axis = 1)\n",
    "    elif len(cat_features) == 0:\n",
    "        X = X_con\n",
    "    elif len(con_features) == 0:\n",
    "        X = X_cat\n",
    "    # return the encoded features\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_usr = process_features(df_users, user_con_features, user_cat_features)\n",
    "X_item = process_features(df_items, item_features, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossLayer(Layer):\n",
    "    \"\"\"\n",
    "        Layer object to minimise the triplet loss.\n",
    "        Here we implement the Bayesian Personal Ranking triplet loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def bpr_triplet_loss(self, inputs):\n",
    "        \"\"\"\n",
    "            Bayesian Personal Ranking triplet loss.\n",
    "            We make use of log-loss for numerical purposes.\n",
    "        \"\"\"\n",
    "        anchor, positive, negative = inputs\n",
    "        p_score = K.dot(anchor,K.transpose(positive))\n",
    "        n_score = K.dot(anchor,K.transpose(negative))\n",
    "        return K.log(1.0 - K.sigmoid(p_score - n_score))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.bpr_triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "class ScoreLayer(Layer):\n",
    "    \"\"\"\n",
    "        Layer object to predict positive matches.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScoreLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def rec_similarity(self, inputs):\n",
    "        \"\"\"\n",
    "            rec_similarity function\n",
    "        \"\"\"\n",
    "        anchor, item = inputs\n",
    "        score = K.dot(anchor,K.transpose(item))\n",
    "        return score\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        pred = self.rec_similarity(inputs)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_users, n_items, emb_dim = 30):\n",
    "    n_user_features = 3\n",
    "    n_item_features = 18\n",
    "\n",
    "    ### Input Layers\n",
    "\n",
    "    user_input = Input((n_user_features,), name='user_input')\n",
    "    positive_item_input = Input((n_item_features,), name='pos_item_input')\n",
    "    negative_item_input = Input((n_item_features,), name='neg_item_input')\n",
    "\n",
    "    inputs = [user_input, positive_item_input, negative_item_input]\n",
    "\n",
    "    ### Embedding Layers\n",
    "\n",
    "    user_emb = Embedding(n_users, emb_dim, input_length=n_user_features, name='user_emb')\n",
    "    # Positive and negative items will share the same embedding\n",
    "    item_emb = Embedding(n_items, emb_dim, input_length=n_item_features, name='item_emb')\n",
    "    # Layer to convert embedding vectors in the same dimensional vectors\n",
    "    vec_conv64 = Dense(64, name = 'dense_vec64', activation = 'relu')\n",
    "    vec_conv32 = Dense(32, name = 'dense_vec32', activation = 'relu')\n",
    "    vec_conv = Dense(emb_dim, name = 'dense_vec', activation = 'softmax')\n",
    "    \n",
    "\n",
    "    # Anchor\n",
    "    a = Flatten(name = 'flatten_usr_emb')(user_emb(user_input))\n",
    "    a = Dense(emb_dim, name = 'dense_user', activation = 'softmax')(a)\n",
    "    \n",
    "    # Positive\n",
    "    p = Flatten(name = 'flatten_pos_emb')(item_emb(positive_item_input))\n",
    "    #p = vec_conv64(p)\n",
    "    p = vec_conv32(p)\n",
    "    p = vec_conv(p)\n",
    "\n",
    "    # Negative\n",
    "    n = Flatten(name = 'flatten_neg_emb')(item_emb(negative_item_input))\n",
    "    #n = vec_conv64(n)\n",
    "    n = vec_conv32(n)\n",
    "    n = vec_conv(n)\n",
    "\n",
    "    #Force the encoding to live on the d-dimentional hypershpere\n",
    "    #a = Lambda(lambda x: K.l2_normalize(x), name = 'normalise_layer_a')(a)\n",
    "    #p = Lambda(lambda x: K.l2_normalize(x), name = 'normalise_layer_p')(p)\n",
    "    #n = Lambda(lambda x: K.l2_normalize(x), name = 'normalise_layer_n')(n)\n",
    "\n",
    "    # Score layers\n",
    "    p_rec_score = ScoreLayer(name='pos_recommendation_score')([a, p])\n",
    "    n_rec_score = ScoreLayer(name='neg_recommendation_score')([a, n])\n",
    "    \n",
    "    # TripletLoss Layer\n",
    "    loss_layer = TripletLossLayer(name='triplet_loss_layer')([a, p, n])\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    network_train = Model(inputs=inputs, outputs=loss_layer, name = 'training_model')\n",
    "\n",
    "    network_predict = Model(inputs=inputs[:-1], outputs=p_rec_score, name = 'inference_model')\n",
    "\n",
    "    # return the model\n",
    "    return network_train, network_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_train, network_predict = build_model(n_users, n_items)\n",
    "optimizer = Adam(lr = 0.001)\n",
    "network_train.compile(loss=None,optimizer=optimizer)\n",
    "network_train.summary()\n",
    "#plot_model(network_train,show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "n_iteration=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_predict.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_train.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Training phase\n",
    "    def cosine_dist(x,y, eps = 1e-6):\n",
    "    return 1 - np.dot(x,y.T)/(np.linalg.norm(x)*np.linalg.norm(y) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(batch_size, X_usr, X_item, df, return_cache = False):\n",
    "            \n",
    "    # constant values\n",
    "    n_user_features = X_usr.shape[1]\n",
    "    n_item_features = X_item.shape[1]\n",
    "\n",
    "    # define user_list\n",
    "    user_list = list(df.index.values)\n",
    "\n",
    "    # initialise result\n",
    "    triplets = [np.zeros((batch_size, n_user_features)), # anchor\n",
    "                np.zeros((batch_size, n_item_features)), # pos\n",
    "                np.zeros((batch_size, n_item_features))  # neg\n",
    "                ]\n",
    "    user_ids = []\n",
    "    p_ids = []\n",
    "    n_ids = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # pick one random user for anchor\n",
    "        anchor_id = random.choice(user_list)\n",
    "        user_ids.append(anchor_id) \n",
    "\n",
    "        # all possible positive/negative samples for selected anchor\n",
    "        p_item_ids = get_pos(df, anchor_id)\n",
    "        n_item_ids = get_neg(df, anchor_id)\n",
    "\n",
    "        # pick one of the positve ids\n",
    "        try:\n",
    "            positive_id = random.choice(p_item_ids)\n",
    "        except IndexError:\n",
    "            positive_id = 0\n",
    "\n",
    "        p_ids.append(positive_id)\n",
    "\n",
    "        # pick one of the negative ids\n",
    "        try:\n",
    "            negative_id = random.choice(n_item_ids)\n",
    "        except IndexError:\n",
    "            negative_id = 0\n",
    "        \n",
    "        n_ids.append(negative_id)\n",
    "\n",
    "        # define triplet\n",
    "        triplets[0][i,:] = X_usr[anchor_id-1][:]\n",
    "        \n",
    "        if positive_id == 0:\n",
    "            triplets[1][i,:] = np.zeros((n_item_features,))\n",
    "        else:\n",
    "            triplets[1][i,:] = X_item[positive_id-1][:]\n",
    "        \n",
    "        if negative_id == 0:\n",
    "            triplets[2][i,:] = np.zeros((n_item_features,))\n",
    "        else:\n",
    "            triplets[2][i,:] = X_item[negative_id-1][:]\n",
    "\n",
    "    if return_cache:\n",
    "        cache = {'users': user_ids, 'positive': p_ids, 'negative': n_ids}\n",
    "        return triplets, cache\n",
    "    \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets_hard(batch_size, X_usr, X_item, df, return_cache = False):\n",
    "    # constant values\n",
    "    n_user_features = X_usr.shape[1]\n",
    "    n_item_features = X_item.shape[1]\n",
    "\n",
    "    # define user_list\n",
    "    user_list = list(df.index.values)\n",
    "\n",
    "    # initialise result\n",
    "    triplets = [np.zeros((batch_size, n_user_features)), # anchor\n",
    "                np.zeros((batch_size, n_item_features)), # pos\n",
    "                np.zeros((batch_size, n_item_features))  # neg\n",
    "                ]\n",
    "    user_ids = []\n",
    "    p_ids = []\n",
    "    n_ids = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # pick one random user for anchor\n",
    "        anchor_id = random.choice(user_list)\n",
    "        user_ids.append(anchor_id) \n",
    "        \n",
    "        # all possible positive/negative samples for selected anchor\n",
    "        p_item_ids = get_pos(df, anchor_id)\n",
    "        n_item_ids = get_neg(df, anchor_id)\n",
    "        \n",
    "        # pick one of the positve ids\n",
    "        try:\n",
    "            positive_id = random.choice(p_item_ids)\n",
    "        except IndexError:\n",
    "            positive_id = 0\n",
    "\n",
    "        p_ids.append(positive_id)\n",
    "        \n",
    "        # pick the most similar negative id\n",
    "        try:\n",
    "            n_min = np.argmin([(cosine_dist(X_item[positive_id-1], X_item[k-1])) for k in n_item_ids])\n",
    "            negative_id = n_item_ids[n_min]\n",
    "        except:\n",
    "            try:\n",
    "                negative_id = random.choice(n_item_ids)\n",
    "            except IndexError:\n",
    "                negative_id = 0\n",
    "            \n",
    "        n_ids.append(negative_id)\n",
    "        \n",
    "        # define triplet\n",
    "        triplets[0][i,:] = X_usr[anchor_id-1][:]\n",
    "        \n",
    "        if positive_id == 0:\n",
    "            triplets[1][i,:] = np.zeros((n_item_features,))\n",
    "        else:\n",
    "            triplets[1][i,:] = X_item[positive_id-1][:]\n",
    "        \n",
    "        if negative_id == 0:\n",
    "            triplets[2][i,:] = np.zeros((n_item_features,))\n",
    "        else:\n",
    "            triplets[2][i,:] = X_item[negative_id-1][:]\n",
    "\n",
    "    if return_cache:\n",
    "        cache = {'users': user_ids, 'positive': p_ids, 'negative': n_ids}\n",
    "        return triplets, cache\n",
    "    \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_triplets_hard(1, X_usr, X_item, df_matrix, return_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 100 # interval for evaluating on one-shot tasks\n",
    "batch_size = 32\n",
    "n_iter = 10000 # No. of training iterations\n",
    "n_val = 1000 # how many one-shot tasks to validate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"Starting ....\")\n",
    "t_start = time.time()\n",
    "\n",
    "for i in range(1, n_iter+1):\n",
    "    triplets = get_triplets_hard(batch_size, X_usr, X_item, df_matrix)\n",
    "    loss = network_train.train_on_batch(triplets, None)\n",
    "    n_iteration += 1\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"[{3}] Time for {0} iterations: {1:.1f} mins, Train Loss: {2}\".format(i, (time.time()-t_start)/60.0,loss,n_iteration))\n",
    "\n",
    "# serialize weights to HDF5\n",
    "network_train.save_weights(\"network_train.h5\")\n",
    "network_predict.save_weights(\"network_predict.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evolution\n",
    "network_predict.predict([X_usr[:1], X_item[7:8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_predict.predict([X_usr[:1], X_item[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probs(network,X_usr,X_item, thrs = 0.5):\n",
    "    m = X_usr.shape[0]\n",
    "    nbevaluation = int(m**2)\n",
    "    probs = np.zeros((nbevaluation))\n",
    "    y = np.zeros((nbevaluation))\n",
    "    \n",
    "    #For each user and item of our dataset\n",
    "    k = 0\n",
    "    for i in range(1,m):\n",
    "        for j in range(1,m):\n",
    "            probs[k] = float(network.predict([X_usr[i-1:i], X_item[j-1:j]]))\n",
    "            if probs[k] > thrs:\n",
    "                y[k] = 1\n",
    "            else:\n",
    "                y[k] = 0    \n",
    "    return probs,y\n",
    "#probs,yprobs = compute_probs(network,x_test_origin[:10,:,:,:],y_test_origin[:10])\n",
    "\n",
    "def compute_metrics(probs,yprobs):\n",
    "    # calculate AUC\n",
    "    auc = roc_auc_score(yprobs, probs)\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(yprobs, probs)\n",
    "    \n",
    "    return fpr, tpr, thresholds,auc\n",
    "\n",
    "def compute_interdist(network):\n",
    "    res = np.zeros((nb_classes,nb_classes))\n",
    "    \n",
    "    ref_images = np.zeros((nb_classes,img_rows,img_cols,1))\n",
    "    \n",
    "    #generates embeddings for reference images\n",
    "    for i in range(nb_classes):\n",
    "        ref_images[i,:,:,:] = dataset_test[i][0,:,:,:]\n",
    "    ref_embeddings = network.predict(ref_images)\n",
    "    \n",
    "    for i in range(nb_classes):\n",
    "        for j in range(nb_classes):\n",
    "            res[i,j] = dist(ref_embeddings[i],ref_embeddings[j])\n",
    "    return res\n",
    "\n",
    "def draw_interdist(network,n_iteration):\n",
    "    interdist = compute_interdist(network)\n",
    "    \n",
    "    data = []\n",
    "    for i in range(nb_classes):\n",
    "        data.append(np.delete(interdist[i,:],[i]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Evaluating embeddings distance from each other after {0} iterations'.format(n_iteration))\n",
    "    ax.set_ylim([0,3])\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Distance')\n",
    "    ax.boxplot(data,showfliers=False,showbox=True)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.xticks(locs,np.arange(nb_classes))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def find_nearest(array,value):\n",
    "    idx = np.searchsorted(array, value, side=\"left\")\n",
    "    if idx > 0 and (idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx])):\n",
    "        return array[idx-1],idx-1\n",
    "    else:\n",
    "        return array[idx],idx\n",
    "    \n",
    "def draw_roc(fpr, tpr,thresholds):\n",
    "    #find threshold\n",
    "    targetfpr=1e-3\n",
    "    _, idx = find_nearest(fpr,targetfpr)\n",
    "    threshold = thresholds[idx]\n",
    "    recall = tpr[idx]\n",
    "    \n",
    "    \n",
    "    # plot no skill\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.title('AUC: {0:.3f}\\nSensitivity : {2:.1%} @FPR={1:.0e}\\nThreshold={3})'.format(auc,targetfpr,recall,abs(threshold) ))\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "probs,yprob = compute_probs(network_predict,X_usr[:100],X_item[:100], thrs=0.3)\n",
    "fpr, tpr, thresholds,auc = compute_metrics(probs,yprob)\n",
    "draw_roc(fpr, tpr,thresholds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
